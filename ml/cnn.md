# 畳み込みニューラルネットワーク
## 活用例
+ 画像や動画に表示されている物体が何がを判定する.
+ 画像のどこに物体があるかを検出する.
+ 学習した特徴量をもとに画像を生成.

## 畳み込みネットワークを構成するもの
### 畳み込み層
#### 畳み込み層とは
[参考記事](https://deepage.net/deep_learning/2016/11/07/convolutional_neural_network.html)
入力値とカーネルフィルタの重なりあった部分を出力する.
入力値にカーネルフィルタを重ねる.少しずつずらして畳み込む. 重なった部分の和が畳み込み層の出力.
具体的な計算
要素積の和
https://deepage.net/deep_learning/2016/11/07/convolutional_neural_network.html#convolutional-neural-network%E3%81%AE%E7%89%B9%E5%BE%B4

合成性
CNNはそれぞれの構成要素を理解すると、パズルのように組み合わせてつくることができるようになる。

各層は次の層へと意味のあるデータを順に受渡していく。層が進むにつれて、ネットワークはより高レベルな特徴を学習していける。

音声認識を例にとると、最初の層では局所的な音量や周波数の変化を検出し、次の層でそれらを組み合わせて騒音やノイズを検出し、より深くなるにつれて人間の声や性別を検出できるようになっているかもしれない。

移動不変性
前述の畳み込みの例で見たとおり、局所領域からフィルタを通して検出していくので物体の位置のズレに頑健になる。

つまり、特徴を検知する対象が入力データのどこにあっても検知することができる。これを移動不変性という。

回転や拡大・縮小に対する不変性はどうなのだろう？ある程度は不変性を維持できるものの、それほど頑健ではないので、データ拡張でそういったデータを増やして学習するなど工夫が必要だ。

#### ゼロパディング
畳み込むと特徴マップの大きさが必ず小さくなってしまう.
もとの入力特徴マップの回りに0を並べることで, 出力サイズが小さくなることを抑えられる.

#### ストライド
フィルタをいくつずつずらすかを表す指標.

#### 入力サイズと出力サイズ
出力サイズ = (入力サイズ + 2 * ゼロパディング - カーネルサイズ) / ストライド + 1

#### パラメータの初期化
+ 重み (カーネルサイズ, 入力サイズ, 出力サイズ)
+ バイアス (出力サイズ)

### プーリング層
特徴マップを小さくするための層.
+ Maxプーリング フィルタ内の最大値を抽出する.
+ Averageプーリング フィルタ内の平均値を抽出する.
+ 微小な位置変化に対して頑健となる
+ ある程度過学習を抑制する
+ 計算コストを下げる

### 全結合層
#### パラメータの初期化
+ 重み 入力サイズ * 出力サイズ

### 出力層
出力チャンネル数はクラス数と一致する.


## CNNの再利用
CNNでは何層もネットワークを重ねて学習することになる. 一度学習させたモデルに対して別の出力をさせたい場合, 一から学習する必要はない.
CNNの浅い層（下層）では, エッジなどの局所的な特徴を学習しているので, 深い層（上層）のみを再学習すればよい.

## CNNの応用
[参考サイト](http://thunders1028.hatenablog.com/entry/2017/11/01/035609)

### 転置畳み込み層
畳み込み層とは逆に特徴マップを大きくする層.
特徴マップを拡大した上で畳み込みを行う.

### GoogLeNet
[参考](https://qiita.com/yu4u/items/7e93c454c9410c4b5427)
基本的にはこれまで見てきたCNNと同じだが, GoogLeNetには横方向の深さもある. この深さのことをインセプション構造と呼ぶ. 

インセプションモジュールでは，ネットワークを分岐させ， サイズの異なるフィルターによる畳み込みを行った後， それらの出力をつなぎ合わせるという処理を行っている． 特に1 * 1のフィルタを多く使うことで, チャンネル方向のサイズを小さくし, パラメータの削減を行っている．

インセプションモジュールおいてもクラス分類を行い， auxiliary lossを追加することが行われている． これにより， ネットワークの中間層に直接誤差を伝搬させることで， 勾配消失を防ぐとともにネットワークの正則化を実現している．

### VGG
小さいフィルターを持つ畳み込み層を2〜4つ連続して重ね、それをプーリング層でサイズを半分にするというのを繰り返し行う構造が特徴。大きいフィルターで画像を一気に畳み込むよりも小さいフィルターを何個も畳み込む(=層を深くする)方が特徴をより良く抽出できるらしい。バッチ正規化は使わないらしい。


小さいフィルターを持つ畳み込み層を2〜4つ連続して重ね、それをプーリング層でサイズを半分にするというのを繰り返し行う構造が特徴。大きいフィルターで画像を一気に畳み込むよりも小さいフィルターを何個も畳み込む(=層を深くする)方が特徴をより良く抽出できるらしい。バッチ正規化は使わないらしい

### ResNet
ResNetは，通常のネットワークのように，何かしらの処理ブロックによる変換F(x)F(x)を単純に次の層に渡していくのではなく，その処理ブロックへの入力xxをショートカットし，H(x)=F(x)+xH(x)=F(x)+xを次の層に渡すことが行われる．このショートカットを含めた処理単位をresidualモジュールと呼ぶ．ResNetでは，ショートカットを通して，backpropagation時に勾配が直接下層に伝わっていくことになり，非常に深いネットワークにおいても効率的に学習ができるようになった．

まずstrideが (2, 2) の7×7畳み込みを行った後，s=2s=2, z=3z=3のmax poolingを行うことで特徴マップを縮小する．その後は，VGGNetと同様に，同一の出力チャネル数を持つresidualモジュールを複数重ね，その後に特徴マップを半分に縮小しつつ出力チャネル数を2倍にすることを繰り返すことでネットワークが構成される．特徴マップを半分に縮小する処理は，max poolingではなく，各residualモジュールの最初にstrideが (2, 2) の畳み込みを行うことで実現している．またGoogLeNetと同様に全結合層の前はGAPを利用するという方針を取っている．
