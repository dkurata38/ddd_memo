# ニューラルネットワーク
## パラメータの更新方法
### ADAM
一つ一つのパラメータを更新するのではなく, 一気に複数のパラメータを更新する.


### 活性化関数
#### シグモイド関数

#### ReLU関数
入力値が0より小さいときは0を戻し, 0より大きい数は入力値をそのまま戻す.
```
def relu(x):
  return np.maximum(0, x)
```

なお微分すると以下のようになる.
```
class ReLu:
  def forward(self, x):
    self.x = x
    return np.maximum(0, x)

  def backward(self, dx):
    return dx[self.x <= 0] = 0
```

##### ReLu関数の派生
+ Leaky ReLU ReLU関数の傾きに0.01を乗算している.
```
x < 0 => 0
x >= 0 => 0.01 * x
```

+ Parametric ReLU(PReLU) 名前の通り, ReLU関数の傾きにがパラメータ化されている.
```
x < 0 => 0
x >= 0 => a * x
```

+ Exponential Linear Units 
```
x < 0 => 0
x >= 0 => e ** x - 1
```

#### ソフトマックス関数

## マルチタスク学習
[参考](http://yamagensakam.hatenablog.com/entry/20140119/1390130395)

複数の関連するタスク同士で情報を共有することにより、予測精度を向上させようという考え方の枠組み。特徴ベクトルやラベルの定義域はタスク間で共通の場合が多いです。考え方は転移学習と類似しているので、その辺りの論文も参考になります。両者の違いは、マルチタスク学習の方は、タスク間で協調し合うことで全てのタスクの精度を向上させようという目的に対し、転移学習はある目標のタスクがあって、その目標タスクの精度向上が目的である点で違います。
マルチタスク学習の例は、書き手が異なる学習データを用いた手書き文字認識です。学習データに本人が書いたデータが少ない場合、他人の書いた文字データを利用することで精度を向上させようという試みは、マルチタスク学習とみなせます（タスク1はAさんの手書き文字認識器を学習、タスク2はBさんの手書き文字認識器を学習・・・）。それ以外にも、テキストデータを分類する問題で分野の違うテキストデータを用いたり、シーンの異なる画像データで物体検出を行ったりといった場面でマルチタスク学習が適用できます。
手法としては、異なるタスクのサンプルを学習する際、そのサンプルがどれだけ適合するかという重みを付けるアプローチや、正則化項などを用いてタスク間でパラメータが類似するように学習するアプローチなどがあります。例えば、前者には、タスク間の関係が共変量シフトという状況下であると仮定し確率密度比を重みづけした手法が、Covariate Shift Adaptation by Importance Weighted Cross Validation等の論文で提案されています*2。後者も行列ノルムの正則化を利用し、各タスクのパラメータがスパース、かつ、0になる要素がタスク間で共通（jointly sparse）になるように学習する手法Robust visual tracking via multi-task sparse learning等々様々な手法があります。また、上述のマルチビュー学習とマルチタスク学習の問題設定を組み合わせたA Graph-based Framework for Multi-Task Multi-View Learningなんてものもあります。
サーベイ論文としては、転移学習のサーベイですがA Survey on Transfer Learningが有名でしょう。検索すれば日本語の資料なんかもたくさん出てきます。
