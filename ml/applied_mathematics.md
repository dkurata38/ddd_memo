# 応用数学
## 情報量
### エントロピー
あるできごと（事象）が起きた際, それがどれほど起こりにくいかを表す尺度である。
あくまでそのできごとの起こりにくさ（確率）だけによって決まる数学的な量でしかなく, 個人・社会における有用性とは無関係である。
事象Eが起こる確率をP(E)とするとき, 事象Eが起こったことを知らされたとき受け取る（選択）情報量I(E) を
```
I(E) = log(1 / P(E)) = -logP(E)
```
たいてい, 対数の底は2で、自然対数であることが多い.

### シャノンエントロピー
シャノンエントロピーとはエントロピーの期待値であり, 加重平均である.
シャノンエントロピーとは`P(E) * I(E)`の総和である.

### KLダイバージェンス
確率分布の差異を表す事から、カルバック・ライブラー距離 と呼ばれる事もあるが、距離の公理を満たさないので、数学的な意味での距離ではない。