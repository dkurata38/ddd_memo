# 応用数学
## 情報量
### エントロピー
あるできごと（事象）が起きた際, それがどれほど起こりにくいかを表す尺度である。
あくまでそのできごとの起こりにくさ（確率）だけによって決まる数学的な量でしかなく, 個人・社会における有用性とは無関係である。
事象Eが起こる確率をP(E)とするとき, 事象Eが起こったことを知らされたとき受け取る（選択）情報量I(E) を
```
I(E) = log(1 / P(E)) = -logP(E)
```
たいてい, 対数の底は2で、自然対数であることが多い.

### シャノンエントロピー
シャノンエントロピーとはエントロピーの期待値であり, 加重平均である.
シャノンエントロピーとは`P(E) * I(E)`の総和である.

### KLダイバージェンス
 [生成モデルで語られる Kullback-Leibler を理解する](https://qiita.com/TomokIshii/items/b9a11c19bd5c36ad0287)
 
確率分布の差異を表す事から、カルバック・ライブラー距離 と呼ばれる事もあるが、距離の公理を満たさないので、数学的な意味での距離ではない。
具体的に言うと, *非負性*は満たすものの, *対称性*`KL(P(x)|Q(x)) = KL(Q(x)|P(x))`は満たさない.

確率分布がほとんど同じところで一致する場合は, KLダイバージェンスの値が0に近づき, 一致しなければしないほど値が大きくなる.

## 尤度
[参考資料](http://web.econ.keio.ac.jp/staff/bessho/lecture/111012ML.pdf)
最尤推定量（ML: Maximum Likelihood estimator）は M 推定量の代表例である．
標本に含まれる観測値 wi が互いに独立に同一の分布に従う（i.i.d.: independently and
identically distributed）とし，その分布が有限のパラメタベクトル θ で表現できるとしよ
う．wi の確率密度関数が f(wi
; θ) と表され，f(., .) の関数形は既知とする．wi が i.i.d. だ
から，f(., .) の関数形は i に依存しない．θ の真の値を θ0 と書けば，wi が観測される真の
確率密度は f(wi
; θ0) である．θ0 ∈ Θ のとき，モデルは correctly specified である，という．
標本 (w1, w2, ..., wn) が観測される同時確率密度（joint density）は，wi が i.i.d. だから，
f(w1, w2, ..., wn; θ) = ∏n
i=1
f(wi
; θ0) (7.1.4)
である．さて，手許にある標本は「最も起きやすい状況」が起きた結果，と考えることは推
定の発想として自然なものだろう．そのためには，関数形 f が分かっているのだから，標
本 (w1, w2, ..., wn) を固定しておいて，パラメタベクトル θ を動かして，この同時確率密度
を最大にするような θ を見つければよい．同時確率密度をパラメタベクトル θ の関数と捉
え直すとき，この関数を尤度関数（likelihood function）と呼ぶ．対数変換は単調変換であ
るから，尤度関数を最大化する θ と，対数変換した尤度関数を最大化する θ は一致する．す
なわち，最尤推定量とは，次の対数尤度関数（log likelihood function）を最大化する．
log f(w1, w2, ..., wn; θ) = ∑n
i=1
log f(wi
; θ0) (7.1.5)
ここで，
m(wi
; θ) = log f(wi
; θ0) 

### 条件付き尤度
ここまで，観測値を表すベクトルをまとめて wi と呼んできた．しかし，ほとんどの応用
では，最小 2 乗推定のときのように，被説明変数 yi と説明変数 xi を考え，説明変数の変
化が被説明変数の条件付き分布に与える効果を検討している．最尤推定法においては，ベ
クトル wi を 1 つの被説明変数 yi と複数の説明変数 xi に分割して考える必要性は必ずしも
ないが，被説明変数と説明変数に分けて考えるほうが簡単なことも多い．
そこで，説明変数 xi の条件付きの被説明変数 yi の確率密度関数を f(yi|xi; θ) とし，説明変数ベクトル xi の周辺密度関数を f(xi; ψ) と書こう．すると，条件付き確率密度の関係式から，f(yi, xi; θ, ψ) = f(yi|xi; θ)f(xi; ψ) (7.1.9)
となる．いま，θ と ψ に関係がないとすると対数尤度は，
∑ni=1log f(wi; θ, ψ) = ∑ni=1log f(yi|xi; θ) +∑ni=1log f(xi; ψ) (7.1.10)
と分解できる．

θ の値だけに興味があるときには，右辺の第 2 項の最大化問題の解が第 1項の解に関係しないかぎり，第 2 項のことを考えなくてもよい．つまり，
m(wi; θ) = log f(yi|xi; θ0) (7.1.11)
とおいた M 推定量を考えればよい．
